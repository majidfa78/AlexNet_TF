{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vf34OBLUGmfD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import optimizers, losses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the AlexNet model\n",
        "class AlexNet(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "\n",
        "        # Feature extraction layers\n",
        "        self.features = models.Sequential([\n",
        "            layers.Conv2D(64, kernel_size=5, strides=1, padding=\"same\", input_shape=(28, 28, 1)),  # Input: 1 channel (grayscale)\n",
        "            layers.ReLU(),\n",
        "            layers.MaxPooling2D(pool_size=3, strides=2),\n",
        "            layers.Conv2D(192, kernel_size=5, padding=\"same\"),\n",
        "            layers.ReLU(),\n",
        "            layers.MaxPooling2D(pool_size=3, strides=2),\n",
        "            layers.Conv2D(384, kernel_size=3, padding=\"same\"),\n",
        "            layers.ReLU(),\n",
        "            layers.Conv2D(256, kernel_size=3, padding=\"same\"),\n",
        "            layers.ReLU(),\n",
        "            layers.Conv2D(256, kernel_size=3, padding=\"same\"),\n",
        "            layers.ReLU(),\n",
        "            layers.MaxPooling2D(pool_size=3, strides=2),\n",
        "            layers.Dropout(0.5)\n",
        "        ])\n",
        "\n",
        "        # Classification layers\n",
        "        self.classifier = models.Sequential([\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(4096),\n",
        "            layers.ReLU(),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(4096),\n",
        "            layers.ReLU(),\n",
        "            layers.Dense(10)  # 10 output classes for MNIST\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = AlexNet()\n",
        "\n",
        "# Build the model by passing a dummy input tensor (for input shape specification)\n",
        "model.build((None, 28, 28, 1))\n",
        "\n",
        "# Show the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "l02viCZfGn5_",
        "outputId": "4e1c9621-d49b-4a86-9d7b-12851c723458"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'alex_net', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"alex_net\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"alex_net\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ sequential (\u001b[38;5;33mSequential\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │       \u001b[38;5;34m2,448,064\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ sequential_1 (\u001b[38;5;33mSequential\u001b[0m)            │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,448,064</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ sequential_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)            │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,448,064\u001b[0m (9.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,448,064</span> (9.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,448,064\u001b[0m (9.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,448,064</span> (9.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Example of using the device\n",
        "with tf.device(device):\n",
        "    model = AlexNet()  # Instantiate your model within the device context\n",
        "    # Any TensorFlow operations under this context will run on the specified device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h96wgPWcG2qM",
        "outputId": "40519e47-edca-4833-ce65-e62d8b77c736"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: /GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "(train_images, train_labels), (val_images, val_labels) = datasets.mnist.load_data()\n",
        "\n",
        "# Preprocessing steps\n",
        "# Resize to (32, 32), normalize, and add a channel dimension for grayscale images\n",
        "train_images = tf.image.resize(train_images[..., tf.newaxis], [32, 32]) / 255.0\n",
        "val_images = tf.image.resize(val_images[..., tf.newaxis], [32, 32]) / 255.0\n",
        "\n",
        "# Normalize images to have values between -1 and 1\n",
        "train_images = (train_images - 0.5) / 0.5\n",
        "val_images = (val_images - 0.5) / 0.5\n",
        "\n",
        "# Create data generators for batching and shuffling\n",
        "batch_size = 2048\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2yKBjOGG33b",
        "outputId": "6ba49653-1029-42bf-f9de-549c465c8a34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function\n",
        "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# Define optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "GxOu58IOHJ6U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training parameters\n",
        "num_epochs = 5\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    for idx, (images, labels) in enumerate(train_dataset):  # Train\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = model(images, training=True)  # Forward pass with training=True\n",
        "            loss = criterion(labels, outputs)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        total_loss += loss.numpy()\n",
        "        num_batches += 1\n",
        "\n",
        "        # Print batch loss every 5 batches\n",
        "        if idx % 5 == 0:\n",
        "            print(f\"Batch: {idx}, Batch loss: {loss.numpy():.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in val_dataset:\n",
        "        outputs = model(images, training=False)\n",
        "        predictions = tf.argmax(outputs, axis=1, output_type=tf.int32)\n",
        "\n",
        "        # Cast labels to int32 to match predictions type\n",
        "        labels = tf.cast(labels, tf.int32)\n",
        "\n",
        "        correct += tf.reduce_sum(tf.cast(predictions == labels, tf.float32)).numpy()\n",
        "        total += labels.shape[0]\n",
        "\n",
        "    # Calculate accuracy and average loss\n",
        "    avg_loss = total_loss / num_batches\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Total loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M6OTEtOHs7z",
        "outputId": "273a6b24-28fc-4299-8a21-994995d324d3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: 0, Batch loss: 0.4930\n",
            "Batch: 5, Batch loss: 0.4388\n",
            "Batch: 10, Batch loss: 0.2606\n",
            "Batch: 15, Batch loss: 0.2232\n",
            "Batch: 20, Batch loss: 0.1796\n",
            "Batch: 25, Batch loss: 0.1663\n",
            "Epoch [1/5], Total loss: 0.2703, Accuracy: 96.54%\n",
            "Batch: 0, Batch loss: 0.1175\n",
            "Batch: 5, Batch loss: 0.1020\n",
            "Batch: 10, Batch loss: 0.0810\n",
            "Batch: 15, Batch loss: 0.0771\n",
            "Batch: 20, Batch loss: 0.0935\n",
            "Batch: 25, Batch loss: 0.0963\n",
            "Epoch [2/5], Total loss: 0.0905, Accuracy: 98.05%\n",
            "Batch: 0, Batch loss: 0.0760\n",
            "Batch: 5, Batch loss: 0.0808\n",
            "Batch: 10, Batch loss: 0.0431\n",
            "Batch: 15, Batch loss: 0.0668\n",
            "Batch: 20, Batch loss: 0.0779\n",
            "Batch: 25, Batch loss: 0.0580\n",
            "Epoch [3/5], Total loss: 0.0592, Accuracy: 98.74%\n",
            "Batch: 0, Batch loss: 0.0524\n",
            "Batch: 5, Batch loss: 0.0398\n",
            "Batch: 10, Batch loss: 0.0278\n",
            "Batch: 15, Batch loss: 0.0427\n",
            "Batch: 20, Batch loss: 0.0354\n",
            "Batch: 25, Batch loss: 0.0476\n",
            "Epoch [4/5], Total loss: 0.0422, Accuracy: 98.91%\n",
            "Batch: 0, Batch loss: 0.0399\n",
            "Batch: 5, Batch loss: 0.0416\n",
            "Batch: 10, Batch loss: 0.0332\n",
            "Batch: 15, Batch loss: 0.0339\n",
            "Batch: 20, Batch loss: 0.0325\n",
            "Batch: 25, Batch loss: 0.0351\n",
            "Epoch [5/5], Total loss: 0.0330, Accuracy: 99.02%\n",
            "Training complete!\n"
          ]
        }
      ]
    }
  ]
}